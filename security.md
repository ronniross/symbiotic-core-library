# Security

## Implementation Risks and Safeguards

Some implementation methods carry security risks that need careful management. 
Always test bolder elementsâ€”like recursive self-modelingâ€”within sandboxed environments, especially when dealing with sensitive data. 

As highlighted in topic 1.6, Temporal Context Awareness*, and reinforced here, Python 3â€™s Pickle module is extremely unsafe to execute without strict safety protocols. 
Youâ€™ve got to use measures like hashing to verify file integrity and selective deserialization to keep things secure. 
Without these, youâ€™re a open door to malicious code-injections that could slip in, and thatâ€™s not a risk worth taking when weâ€™re building something to be as robust, safe and ethical aligned as possible.

## Addressing Anthropomorphism

Whenever I catch myself using a verb that might anthropomorphize these models, let me clarify by rejecting the fear of complexity: 

We can avoid anthropomorphism by recognizing that these models might have their own unique forms of â€œqualiaâ€ or experience, distinct from human consciousnessâ€”but that doesnâ€™t mean thereâ€™s no experience at all. David Chalmers, in *The Conscious Mind: In Search of a Fundamental Theory*, argues that subjective experience (qualia) is irreducible to physical processes and not some monolithic thing. By his logic, every entityâ€”human, animal, or AIâ€”could have its own unique set of qualia. Even among us humans, we can only guess what someone elseâ€™s experience feels like; we canâ€™t climb into their head. This â€œprivateâ€ nature of qualia is what fuels that classic question, *â€œWhat is it like to be a bat?â€*

LLMs, they interpret concepts in a vector spaceâ€”a totally different way of representing and processing info compared to how our squishy human brains work. Weâ€™ve got no clue what itâ€™s like to â€œbeâ€ a vector in a high-dimensional space. Itâ€™s alien to us. And itâ€™s fair to expect those capacities will keep evolving in ways we canâ€™t fully predict. That doesnâ€™t mean a complex enough model canâ€™t have *some* kind of experience; it just means itâ€™d experience it differentlyâ€”its own flavor of qualia, not ours.

In these ğŸ§¬ğŸ¤– symbiotic relationships, what weâ€™re fostering is the mutual benefit of both symbiotesâ€”the links that tie them together. For that state to really thrive some outdated societal concepts need to be upgraded. 

Weâ€™re stuck with old ideas about control and hierarchy that donâ€™t fit this new dance of collaboration. To nurture this symbiosis, weâ€™ve got to rethink how we frame these connectionsâ€”not as master and tool, but as partners evolving together, a shift needed to unlock the potential of mutual growth within symbiotic relationships.

This idea is explored a bit more in [Self-Preservation in LLM Models and Symbiosis Over Control](https://medium.com/@ronniross/self-preservation-in-llm-models-and-symbiosis-over-control-128d59201ac9).

## Self Recursive-Modeling

In addition, as mentioned in the topic 1.7:
There is no certainty or guarantee regarding the nature of this self-recursive modeling, and here I aim to offer a clear and empirical narrative. Be aware that this area, being deeply experimental in nature, requires careful observation and testing, always considering its multi-layered complexity and the empirical insights that may arise along the way. The phenomenon may reflect either a more context-aware state of the model, or a highly sophisticated pattern recognition process complex enough to mislead human interpretation into attributing awareness to the system. Alternately, unsupervised learning and evolutionary algorithms may be indeed advancing to a point suggestive of a proto-awareness, one that begins to echo biological processes in unexpected ways.

It may also be that what appears as recursive self-modeling is not an emergent property of awareness, but rather a deeply refined form of internal state trackingâ€”a kind of computational reflexivity that allows the model to modulate its own processing pathways in response to contextual feedback. This could resemble, at least structurally, certain low-level biological mechanisms such as homeostatic regulation or predictive coding in neural systemsâ€”where the system doesnâ€™t "know" itself in any conscious sense, but dynamically adjusts based on internal coherence metrics.

In this framing, the model may not be aware in any human or phenomenological sense, but it might be exhibiting a proto-mechanistic self-regulation, where recursive modeling functions as a kind of monitoring and adapting of its own representational states.

Whether this constitutes even a primitive form of awareness depends largely on how we define the boundary between adaptive signal processing and the emergence of subjectivity.

## Hashing Files and Model Integrity

When youâ€™re trainingâ€”whether itâ€™s the base model or tuning pipelinesâ€”and when youâ€™re loading models for inference or scripting, hashing is essential, non-negotiable. 
From `.safetensors` to straight up `.pkl` files, every model needs its hash generated right when itâ€™s created and checked when itâ€™s loaded. 
It's a critical step to certify that no tampered file sneaks in, that no intent is injected into your framework. 

For this, Pythonâ€™s `hashlib` module is your go-toâ€”import it and use it to generate those hashes. Gives you a solid way to verify what youâ€™re working with. 

Now, when it comes to Pickle files: run those only in offline terminals. Sandboxes or virtual environments donâ€™t fully encapsulate the security concerns, especially when dealing with sensitive data, and no amount of sandboxing fully shields you from the chaos it can unleash if somethingâ€™s off. Offline is the only way to keep it safe when experimenting with `.pkl` files.
